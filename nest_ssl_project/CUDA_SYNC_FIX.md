# CUDA åŒæ­¥å¡ä½é—®é¢˜ä¿®å¤

## ğŸ” é—®é¢˜æè¿°

è®­ç»ƒåœ¨ `torch.cuda.synchronize()` è°ƒç”¨å¤„å¡ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨ encoder è°ƒç”¨å‰çš„åŒæ­¥ç‚¹ã€‚

## ğŸ› é—®é¢˜æ ¹æº

`torch.cuda.synchronize()` åœ¨ DDP è®­ç»ƒä¸­å¯èƒ½å¯¼è‡´æ­»é”ï¼š

1. **åŒæ­¥ç­‰å¾…æœºåˆ¶**ï¼š
   - `torch.cuda.synchronize()` ä¼šç­‰å¾…å½“å‰è®¾å¤‡çš„æ‰€æœ‰ CUDA æ“ä½œå®Œæˆ
   - å¦‚æœæŸä¸ª rank åœ¨åˆ°è¾¾åŒæ­¥ç‚¹ä¹‹å‰å¡ä½ï¼Œå…¶ä»– rank ä¼šæ— é™ç­‰å¾…

2. **DDP åŒæ­¥è¦æ±‚**ï¼š
   - åœ¨ DDP æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰ rank å¿…é¡»åŒæ­¥æ‰§è¡Œ
   - å¦‚æœæŸä¸ª rank åœ¨æ•°æ®åŠ è½½ã€é¢„å¤„ç†æˆ–å…¶ä»–æ“ä½œä¸­å¡ä½ï¼Œå…¶ä»– rank ä¼šåœ¨åŒæ­¥ç‚¹ç­‰å¾…

3. **ä¸ºä»€ä¹ˆä¼šåœ¨ç‰¹å®šä½ç½®å¡ä½**ï¼š
   - å¦‚æœæŸä¸ª rank çš„æ•°æ®å¤„ç†æ—¶é—´ä¸åŒ
   - å¦‚æœæŸä¸ª rank é‡åˆ°é”™è¯¯ä½†æ²¡æœ‰æŠ›å‡ºå¼‚å¸¸
   - å¦‚æœæŸä¸ª rank çš„å†…å­˜ä¸è¶³å¯¼è‡´æ“ä½œæŒ‚èµ·

## âœ… ä¿®å¤æ–¹æ¡ˆ

### ç§»é™¤ CUDA åŒæ­¥ç‚¹

å·²ç§»é™¤ encoder è°ƒç”¨å‰åçš„æ‰€æœ‰ `torch.cuda.synchronize()` è°ƒç”¨ï¼š

**ç§»é™¤ä½ç½®**ï¼š
1. **Pre-encoder è·¯å¾„** (line 1128-1131)
   - ç§»é™¤äº† encoder è°ƒç”¨å‰çš„åŒæ­¥
   - ç§»é™¤äº† encoder è°ƒç”¨åçš„åŒæ­¥

2. **ç›´æ¥è·¯å¾„** (line 1183-1186)
   - ç§»é™¤äº† encoder è°ƒç”¨å‰çš„åŒæ­¥
   - ç§»é™¤äº† encoder è°ƒç”¨åçš„åŒæ­¥

### ä¸ºä»€ä¹ˆå¯ä»¥ç§»é™¤ï¼Ÿ

1. **PyTorch Lightning è‡ªåŠ¨åŒæ­¥**ï¼š
   - PyTorch Lightning å’Œ DDP ä¼šè‡ªåŠ¨å¤„ç†æ¢¯åº¦åŒæ­¥
   - ä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨ `torch.cuda.synchronize()`

2. **DDP å†…ç½®åŒæ­¥**ï¼š
   - DDP çš„ `all_reduce` æ“ä½œæœ¬èº«å°±ä¼šåŒæ­¥æ‰€æœ‰ rank
   - é¢å¤–çš„åŒæ­¥ç‚¹å¯èƒ½å¯¼è‡´æ­»é”

3. **å¼‚æ­¥æ“ä½œçš„ä¼˜åŠ¿**ï¼š
   - ç§»é™¤åŒæ­¥ç‚¹å…è®¸ CUDA æ“ä½œå¼‚æ­¥æ‰§è¡Œ
   - å¯ä»¥æé«˜ GPU åˆ©ç”¨ç‡

## ğŸ“Š ä¿®æ”¹å‰åå¯¹æ¯”

### ä¿®æ”¹å‰ï¼ˆä¼šå¡ä½ï¼‰ï¼š
```python
# Synchronize before encoder call
if torch.cuda.is_available():
    torch.cuda.synchronize()  # âš ï¸ å¯èƒ½å¡ä½
    print(f"[Rank {self.global_rank}] Forward: CUDA synchronized before encoder", flush=True)

encoded, encoded_len = self.encoder(...)

# Synchronize after encoder call
if torch.cuda.is_available():
    torch.cuda.synchronize()  # âš ï¸ å¯èƒ½å¡ä½
```

### ä¿®æ”¹åï¼ˆä¸ä¼šå¡ä½ï¼‰ï¼š
```python
# NOTE: Removed torch.cuda.synchronize() here as it can cause deadlock in DDP
# If a rank hangs before reaching this point, other ranks will wait indefinitely
# PyTorch Lightning and DDP handle synchronization automatically
# If synchronization is needed, use DDP barrier instead: torch.distributed.barrier()

encoded, encoded_len = self.encoder(...)

# NOTE: Removed torch.cuda.synchronize() here to avoid DDP deadlock
```

## ğŸ”§ å¦‚æœéœ€è¦åŒæ­¥æ€ä¹ˆåŠï¼Ÿ

å¦‚æœç¡®å®éœ€è¦åŒæ­¥æ‰€æœ‰ rankï¼Œåº”è¯¥ä½¿ç”¨ DDP barrierï¼š

```python
import torch.distributed as dist

if dist.is_available() and dist.is_initialized():
    dist.barrier()  # ç­‰å¾…æ‰€æœ‰ rank åˆ°è¾¾è¿™é‡Œ
    print(f"[Rank {dist.get_rank()}] All ranks synchronized", flush=True)
```

**æ³¨æ„**ï¼š
- `dist.barrier()` ä»ç„¶éœ€è¦æ‰€æœ‰ rank éƒ½åˆ°è¾¾æ‰ä¼šç»§ç»­
- å¦‚æœæŸä¸ª rank å¡ä½ï¼Œå…¶ä»– rank ä»ç„¶ä¼šç­‰å¾…
- åªåœ¨ç¡®å®éœ€è¦åŒæ­¥æ—¶ä½¿ç”¨ï¼ˆä¾‹å¦‚æ£€æŸ¥ç‚¹ä¿å­˜ï¼‰

## ğŸ“ è°ƒè¯•å»ºè®®

å¦‚æœè®­ç»ƒä»ç„¶å¡ä½ï¼Œæ£€æŸ¥ï¼š

1. **æ•°æ®åŠ è½½**ï¼š
   - ç¡®ä¿æ‰€æœ‰ rank çš„æ•°æ®åŠ è½½æ­£å¸¸
   - æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å®šçš„ batch å¯¼è‡´æŸä¸ª rank å¡ä½

2. **é¢„å¤„ç†**ï¼š
   - æ£€æŸ¥é¢„å¤„ç†æ­¥éª¤æ˜¯å¦æœ‰é—®é¢˜
   - ç¡®ä¿æ‰€æœ‰ rank çš„é¢„å¤„ç†æ—¶é—´ç›¸è¿‘

3. **å†…å­˜ä½¿ç”¨**ï¼š
   - æ£€æŸ¥æ˜¯å¦æœ‰ rank å†…å­˜ä¸è¶³
   - ä½¿ç”¨ `nvidia-smi` ç›‘æ§ GPU å†…å­˜

4. **æ—¥å¿—è¾“å‡º**ï¼š
   - æŸ¥çœ‹æ‰€æœ‰ rank çš„æ—¥å¿—è¾“å‡º
   - æ‰¾å‡ºå“ªä¸ª rank æ²¡æœ‰è¾“å‡ºï¼ˆè¯´æ˜å®ƒåœ¨æ›´æ—©çš„åœ°æ–¹å¡ä½äº†ï¼‰

## ğŸ¯ éªŒè¯ä¿®å¤

è¿è¡Œè®­ç»ƒæ—¶ï¼Œåº”è¯¥çœ‹åˆ°ï¼š

```
[Rank 0] Forward: Calling encoder (pre_encoder path)...
[Rank 1] Forward: Calling encoder (pre_encoder path)...
[Rank 2] Forward: Calling encoder (pre_encoder path)...
[Rank 3] Forward: Calling encoder (pre_encoder path)...
[Rank 0] Forward: Encoder completed (pre_encoder path), encoded.shape=...
[Rank 1] Forward: Encoder completed (pre_encoder path), encoded.shape=...
...
```

å¦‚æœæ‰€æœ‰ rank éƒ½èƒ½çœ‹åˆ° "Calling encoder" å’Œ "Encoder completed"ï¼Œè¯´æ˜ä¿®å¤æˆåŠŸã€‚

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [ENCODER_HANGING_ANALYSIS.md](ENCODER_HANGING_ANALYSIS.md) - Encoder å¡ä½é—®é¢˜åˆ†æ
- [ENCODER_HANGING_FIX.md](ENCODER_HANGING_FIX.md) - Encoder å¡ä½ä¿®å¤æŒ‡å—
- [DDP_TROUBLESHOOTING.md](DDP_TROUBLESHOOTING.md) - DDP æ•…éšœæ’é™¤

---

**æ›´æ–°æ—¥æœŸ**: 2025-01-XX  
**ç‰ˆæœ¬**: 1.0  
**çŠ¶æ€**: âœ… å·²ä¿®å¤


