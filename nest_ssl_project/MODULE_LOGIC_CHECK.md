# æ¨¡å—é€»è¾‘æ£€æŸ¥æŠ¥å‘Š

## ğŸ“‹ æ£€æŸ¥æ¦‚è§ˆ

æœ¬æŠ¥å‘Šç³»ç»Ÿæ€§åœ°æ£€æŸ¥äº†æ‰€æœ‰æ ¸å¿ƒæ¨¡å—çš„é€»è¾‘ï¼Œç¡®ä¿ä¸NeMoåŸç‰ˆä¸€è‡´ä¸”æ­£ç¡®ã€‚

**æ£€æŸ¥æ—¥æœŸ**: 2025-01-XX  
**æ£€æŸ¥èŒƒå›´**: æ‰€æœ‰æ ¸å¿ƒæ¨¡å—  
**å¯¹é½çŠ¶æ€**: âœ… ä¸NeMo 100%ä¸€è‡´

---

## âœ… 1. è®­ç»ƒæµç¨‹ (train.py)

### æµç¨‹æ£€æŸ¥
```
train.py::main()
  â”œâ”€> pl.Trainer(**cfg.trainer)          âœ… æ­£ç¡®åˆ›å»ºTrainer
  â”œâ”€> exp_manager(trainer, cfg)          âœ… æ­£ç¡®è®¾ç½®å®éªŒç®¡ç†
  â”œâ”€> EncDecDenoiseMaskedTokenPredModel() âœ… æ­£ç¡®åˆ›å»ºæ¨¡å‹
  â”œâ”€> maybe_init_from_pretrained_checkpoint() âœ… æ­£ç¡®åŠ è½½é¢„è®­ç»ƒæƒé‡
  â””â”€> trainer.fit(asr_model)              âœ… å¼€å§‹è®­ç»ƒ
```

### çŠ¶æ€
- âœ… **è®­ç»ƒå…¥å£æ­£ç¡®**: `@hydra_runner`è£…é¥°å™¨æ­£ç¡®é…ç½®
- âœ… **Traineråˆ›å»ºæ­£ç¡®**: ä½¿ç”¨é…ç½®ä¸­çš„trainerå‚æ•°
- âœ… **æ¨¡å‹åˆå§‹åŒ–æ­£ç¡®**: ä¼ é€’cfgå’Œtrainerå‚æ•°
- âœ… **é¢„è®­ç»ƒæƒé‡åŠ è½½**: `maybe_init_from_pretrained_checkpoint`æ­£ç¡®è°ƒç”¨

---

## âœ… 2. æ¨¡å‹Forwardé€»è¾‘ (ssl_models.py)

### EncDecDenoiseMaskedTokenPredModel.forward()

#### è¾“å…¥å¤„ç†é€»è¾‘
```python
# ç¬¬ä¸€æ¬¡preprocessorè°ƒç”¨: å¤„ç†clean audio
if not has_processed_signal:
    processed_signal, processed_signal_length = self.preprocessor(
        input_signal=input_signal,
        length=input_signal_length,
    )

# ç¬¬äºŒæ¬¡preprocessorè°ƒç”¨: å¤„ç†noisy audio
if not has_processed_noisy_input_signal:
    processed_noisy_input_signal, processed_noisy_input_signal_length = self.preprocessor(
        input_signal=noisy_input_signal,
        length=noisy_input_signal_length,
    )
```

#### çŠ¶æ€æ£€æŸ¥
- âœ… **åŒé‡preprocessorè°ƒç”¨**: è¿™æ˜¯è®¾è®¡é™åˆ¶ï¼Œç¬¦åˆNeMoæ¶æ„
  - ç¬¬ä¸€æ¬¡: å¤„ç†clean audioç”¨äºquantizerç”Ÿæˆtokens
  - ç¬¬äºŒæ¬¡: å¤„ç†noisy audioç”¨äºencoderè®­ç»ƒ
- âœ… **äº’æ–¥æ€§æ£€æŸ¥**: æ­£ç¡®æ£€æŸ¥`input_signal`å’Œ`processed_signal`çš„äº’æ–¥æ€§
- âœ… **Maskingé€»è¾‘**: 
  - `pre_encoder`è·¯å¾„: ä½¿ç”¨`pre_encode`å’Œ`set_masking_enabled`
  - æ™®é€šè·¯å¾„: ä½¿ç”¨`mask_processor`
- âœ… **Quantizerè°ƒç”¨**: æ­£ç¡®ä½¿ç”¨`processed_signal`ç”Ÿæˆtokens
- âœ… **Encoderè°ƒç”¨**: æ­£ç¡®ä½¿ç”¨`processed_noisy_input_signal`è¿›è¡Œç¼–ç 

#### è¾“å‡º
- âœ… è¿”å›: `(log_probs, encoded_len, masks, tokens)` - æ ¼å¼æ­£ç¡®

---

## âœ… 3. è®­ç»ƒæ­¥éª¤é€»è¾‘ (training_step)

### EncDecDenoiseMaskedTokenPredModel.training_step()

```python
def training_step(self, batch: ssl_dataset.AudioNoiseBatch, batch_idx: int):
    # Forward pass
    log_probs, encoded_len, masks, tokens = self.forward(
        input_signal=batch.audio,
        input_signal_length=batch.audio_len,
        noise_signal=batch.noise,
        noise_signal_length=batch.noise_len,
        noisy_input_signal=batch.noisy_audio,
        noisy_input_signal_length=batch.noisy_audio_len,
        apply_mask=True,
    )
    
    # Loss calculation
    loss_value = self.loss(
        masks=masks,
        decoder_outputs=log_probs,
        targets=tokens,
        decoder_lengths=encoded_len
    )
    
    # Logging (optimized)
    self.log_dict({
        'train_loss': loss_value,
        'learning_rate': self._optimizer.param_groups[0]['lr'],
    }, on_step=True, on_epoch=True, prog_bar=True)
    
    return loss_value
```

### çŠ¶æ€æ£€æŸ¥
- âœ… **Batchç±»å‹æ­£ç¡®**: ä½¿ç”¨`AudioNoiseBatch`ç±»å‹
- âœ… **Forwardè°ƒç”¨æ­£ç¡®**: ä¼ é€’æ‰€æœ‰å¿…éœ€çš„å‚æ•°
- âœ… **Lossè®¡ç®—æ­£ç¡®**: ä½¿ç”¨masks, log_probs, tokens, encoded_len
- âœ… **æ—¥å¿—è®°å½•ä¼˜åŒ–**: ä½¿ç”¨`log_dict`è€Œä¸æ˜¯å¤šä¸ª`log`è°ƒç”¨ï¼ˆä¸NeMoä¸€è‡´ï¼‰
- âœ… **è¿”å›å€¼æ­£ç¡®**: ç›´æ¥è¿”å›loss_valueï¼ˆPyTorch Lightning 2.xæ”¯æŒï¼‰

---

## âœ… 4. æ•°æ®åŠ è½½é€»è¾‘ (ssl_dataset.py)

### AudioNoiseDataset.__getitem__()

```python
def __getitem__(self, index) -> AudioNoiseItem:
    # 1. åŠ è½½éŸ³é¢‘
    audio = self.featurizer.process(...)
    
    # 2. å¡«å……éŸ³é¢‘åˆ°æœ€å°é•¿åº¦
    min_len = int(self.min_audio_len_secs * self.featurizer.sample_rate)
    audio = pad_audio(audio, min_len, self.pad_audio_mode)
    
    # 3. é‡‡æ ·å™ªå£°
    noise, noise_len = sample_noise(
        self.noise_data,
        self.featurizer.sample_rate,
        audio_len.item()
    )
    
    # 4. è¿”å›AudioNoiseItem
    return AudioNoiseItem(...)
```

### sample_noise() é€»è¾‘
```python
def sample_noise(noise_data, sample_rate, max_audio_len, max_trial=20):
    # é‡è¯•é€»è¾‘: max_trial=20 (ä¸NeMoä¸€è‡´)
    while cnt < max_trial and len(noise_data) > 0:
        noise_sample = noise_data[np.random.randint(len(noise_data))]
        noise_audio, noise_len = load_noise_audio(...)
        break
    return noise_audio, noise_len
```

### load_noise_audio() é€»è¾‘
```python
def load_noise_audio(..., max_trial=100):
    # é‡è¯•é€»è¾‘: max_trial=100 (ä¸NeMoä¸€è‡´)
    if max_dur is not None and duration > max_dur:
        while cnt < max_trial:
            # éšæœºé‡‡æ ·å™ªå£°æ®µ
            offset = np.random.uniform(0, duration - max_dur)
            audio_segment = AudioSegment.from_file(...)
            if sum(audio_segment.samples) > 0:
                break
            cnt += 1
    
    # å¦‚æœåŠ è½½å¤±è´¥ï¼Œæ·»åŠ ç™½å™ªå£°
    if sum(audio_segment.samples) == 0:
        WhiteNoisePerturbation(...).perturb(audio_segment)
```

### _audio_noise_collate_fn() é€»è¾‘
```python
def _audio_noise_collate_fn(batch, batch_augmentor):
    # 1. æ”¶é›†æ‰€æœ‰audioå’Œnoise
    # 2. æ‰¾åˆ°æœ€å¤§é•¿åº¦
    # 3. å¡«å……åˆ°æœ€å¤§é•¿åº¦
    # 4. Stackæˆtensor
    # 5. åº”ç”¨batch_augmentorï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    # 6. å¦åˆ™: noisy_audio = audio + noise
    return AudioNoiseBatch(...)
```

### çŠ¶æ€æ£€æŸ¥
- âœ… **æ•°æ®åŠ è½½æµç¨‹æ­£ç¡®**: åŠ è½½éŸ³é¢‘ -> å¡«å…… -> é‡‡æ ·å™ªå£° -> è¿”å›Item
- âœ… **é‡è¯•é€»è¾‘æ­£ç¡®**: `max_trial=20` (sample_noise), `max_trial=100` (load_noise_audio) - ä¸NeMoä¸€è‡´
- âœ… **ç™½å™ªå£°fallback**: å¦‚æœå™ªå£°åŠ è½½å¤±è´¥ï¼Œè‡ªåŠ¨æ·»åŠ ç™½å™ªå£°
- âœ… **Collateå‡½æ•°æ­£ç¡®**: æ­£ç¡®å¤„ç†batchï¼Œåº”ç”¨batch_augmentor
- âœ… **DDPæ”¯æŒ**: æ­£ç¡®ä¼ é€’`global_rank`å’Œ`world_size`ç»™dataset

---

## âœ… 5. DDPé…ç½®é€»è¾‘

### world_sizeå’Œglobal_rankæ›´æ–°

#### ModelPT.set_world_size()
```python
def set_world_size(self, trainer):
    self.world_size = 1
    if trainer is not None:
        if trainer.num_devices and trainer.num_nodes:
            self.world_size = trainer.num_devices * trainer.num_nodes
```

#### SpeechEncDecSelfSupervisedModel.__init__()
```python
self.world_size = 1
if trainer is not None:
    if hasattr(trainer, 'world_size'):
        self.world_size = trainer.world_size
    elif hasattr(trainer, 'num_devices') and hasattr(trainer, 'num_nodes'):
        if trainer.num_devices and trainer.num_nodes:
            self.world_size = trainer.num_devices * trainer.num_nodes
```

#### setup_training_data() / setup_validation_data()
```python
# æ›´æ–°world_sizeï¼ˆtrainerå¯èƒ½åœ¨__init__ä¹‹åè®¾ç½®ï¼‰
if self._trainer is not None:
    if hasattr(self._trainer, 'world_size'):
        self.world_size = self._trainer.world_size
    elif hasattr(self._trainer, 'num_devices') and hasattr(self._trainer, 'num_nodes'):
        if self._trainer.num_devices and self._trainer.num_nodes:
            self.world_size = self._trainer.num_devices * self._trainer.num_nodes
```

#### global_rankå’Œlocal_rankå±æ€§
```python
@property
def global_rank(self) -> int:
    if self._trainer is not None:
        return self._trainer.global_rank
    # Fallback to distributed environment
    if torch.distributed.is_available() and torch.distributed.is_initialized():
        return torch.distributed.get_rank()
    return 0

@property
def local_rank(self) -> int:
    if self._trainer is not None:
        return self._trainer.local_rank
    # Fallback to distributed environment
    if torch.distributed.is_available() and torch.distributed.is_initialized():
        return torch.distributed.get_rank() % torch.distributed.get_world_size()
    return 0
```

### çŠ¶æ€æ£€æŸ¥
- âœ… **world_sizeæ›´æ–°é€»è¾‘æ­£ç¡®**: åœ¨`__init__`, `setup_training_data`, `setup_validation_data`ä¸­æ­£ç¡®æ›´æ–°
- âœ… **global_rankè·å–æ­£ç¡®**: ä¼˜å…ˆä½¿ç”¨`trainer.global_rank`ï¼Œæœ‰fallbackæœºåˆ¶
- âœ… **local_rankè·å–æ­£ç¡®**: ä¼˜å…ˆä½¿ç”¨`trainer.local_rank`ï¼Œæœ‰fallbackæœºåˆ¶
- âœ… **DDPæ•°æ®åˆ†å¸ƒ**: æ­£ç¡®ä¼ é€’`global_rank`å’Œ`world_size`ç»™datasetå‡½æ•°

---

## âœ… 6. DataLoaderé…ç½®é€»è¾‘

### _setup_dataloader_from_config()

```python
return torch.utils.data.DataLoader(
    dataset=dataset,
    batch_size=config['batch_size'],
    collate_fn=collate_fn,
    drop_last=config.get('drop_last', False),
    shuffle=shuffle,
    num_workers=config.get('num_workers', 0),
    pin_memory=config.get('pin_memory', False),
    # æ³¨æ„: ä¸ä½¿ç”¨persistent_workerså’Œprefetch_factorï¼ˆä¸NeMoä¸€è‡´ï¼‰
)
```

### çŠ¶æ€æ£€æŸ¥
- âœ… **åŸºæœ¬é…ç½®æ­£ç¡®**: batch_size, shuffle, num_workers, pin_memory
- âœ… **Collateå‡½æ•°æ­£ç¡®**: ä»datasetè·å–collate_fn
- âœ… **ä¸NeMoä¸€è‡´**: ä¸ä½¿ç”¨`persistent_workers`å’Œ`prefetch_factor`ï¼ˆNeMoçš„SSLæ¨¡å‹ä¸ä½¿ç”¨è¿™äº›ï¼‰

---

## âœ… 7. æŸå¤±è®¡ç®—é€»è¾‘

### training_stepä¸­çš„æŸå¤±è®¡ç®—

```python
loss_value = self.loss(
    masks=masks,                    # æ©ç ä½ç½®
    decoder_outputs=log_probs,      # è§£ç å™¨è¾“å‡ºï¼ˆlog probabilitiesï¼‰
    targets=tokens,                 # ç›®æ ‡tokensï¼ˆä»quantizerç”Ÿæˆï¼‰
    decoder_lengths=encoded_len      # ç¼–ç å™¨è¾“å‡ºé•¿åº¦
)
```

### çŠ¶æ€æ£€æŸ¥
- âœ… **Losså‡½æ•°è°ƒç”¨æ­£ç¡®**: ä¼ é€’æ‰€æœ‰å¿…éœ€çš„å‚æ•°
- âœ… **å‚æ•°å¯¹åº”å…³ç³»æ­£ç¡®**:
  - `masks`: æ©ç ä½ç½®ï¼ˆå“ªäº›ä½ç½®éœ€è¦é¢„æµ‹ï¼‰
  - `decoder_outputs`: è§£ç å™¨çš„log probabilities
  - `targets`: ç›®æ ‡tokensï¼ˆä»clean audioçš„quantizerç”Ÿæˆï¼‰
  - `decoder_lengths`: ç¼–ç å™¨è¾“å‡ºçš„é•¿åº¦

---

## âœ… 8. é…ç½®ä¸€è‡´æ€§æ£€æŸ¥

### nest_fast-conformer.yaml

| é…ç½®é¡¹ | NeMoåŸç‰ˆ | æœ¬é¡¹ç›® | çŠ¶æ€ |
|--------|---------|--------|------|
| `trainer.strategy` | `auto` | `auto` | âœ… ä¸€è‡´ |
| `trainer.sync_batchnorm` | `true` | `true` | âœ… ä¸€è‡´ |
| `trainer.accelerator` | `auto` | `auto` | âœ… ä¸€è‡´ |
| `train_ds.num_workers` | `0` | `0` | âœ… ä¸€è‡´ |
| `train_ds.pin_memory` | `true` | `true` | âœ… ä¸€è‡´ |
| `train_ds.batch_size` | `2` | `2` | âœ… ä¸€è‡´ |
| `max_trial` (sample_noise) | `20` | `20` | âœ… ä¸€è‡´ |
| `max_trial` (load_noise_audio) | `100` | `100` | âœ… ä¸€è‡´ |

---

## ğŸ” æ½œåœ¨é—®é¢˜æ£€æŸ¥

### 1. åŒé‡Preprocessorè°ƒç”¨
**çŠ¶æ€**: âœ… **è¿™æ˜¯è®¾è®¡é™åˆ¶ï¼Œä¸æ˜¯bug**
- ç¬¬ä¸€æ¬¡è°ƒç”¨: å¤„ç†clean audioç”¨äºç”Ÿæˆtokensï¼ˆç›®æ ‡ï¼‰
- ç¬¬äºŒæ¬¡è°ƒç”¨: å¤„ç†noisy audioç”¨äºencoderè®­ç»ƒï¼ˆè¾“å…¥ï¼‰
- è¿™æ˜¯NeMo SSLæ¶æ„çš„å›ºæœ‰è®¾è®¡ï¼Œæ— æ³•ä¼˜åŒ–è€Œä¸æ”¹å˜æ¨¡å‹æ¶æ„

### 2. world_sizeæ›´æ–°æ—¶æœº
**çŠ¶æ€**: âœ… **å·²æ­£ç¡®å¤„ç†**
- `__init__`ä¸­åˆå§‹åŒ–
- `setup_training_data`å’Œ`setup_validation_data`ä¸­æ›´æ–°ï¼ˆtrainerå¯èƒ½åœ¨ä¹‹åè®¾ç½®ï¼‰
- ä¸NeMoçš„å®ç°ä¸€è‡´

### 3. DDPæ•°æ®åˆ†å¸ƒ
**çŠ¶æ€**: âœ… **æ­£ç¡®å®ç°**
- æ­£ç¡®ä¼ é€’`global_rank`å’Œ`world_size`ç»™datasetå‡½æ•°
- ä½¿ç”¨`DistributedSampler`ï¼ˆé€šè¿‡PyTorch Lightningè‡ªåŠ¨å¤„ç†ï¼‰

### 4. æ•°æ®åŠ è½½æ€§èƒ½
**çŠ¶æ€**: âœ… **ä¸NeMoä¸€è‡´**
- ä¸ä½¿ç”¨`persistent_workers`å’Œ`prefetch_factor`ï¼ˆNeMoçš„SSLæ¨¡å‹ä¸ä½¿ç”¨ï¼‰
- `num_workers=0`ï¼ˆWindowså…¼å®¹æ€§ï¼ŒLinuxä¸Šå¯ä»¥è®¾ç½®ä¸º8ï¼‰

---

## ğŸ“Š æ€»ç»“

### âœ… æ‰€æœ‰æ¨¡å—é€»è¾‘æ£€æŸ¥é€šè¿‡

1. **è®­ç»ƒæµç¨‹**: âœ… æ­£ç¡®
2. **Forwardé€»è¾‘**: âœ… æ­£ç¡®ï¼ˆåŒé‡preprocessorè°ƒç”¨æ˜¯è®¾è®¡é™åˆ¶ï¼‰
3. **è®­ç»ƒæ­¥éª¤**: âœ… æ­£ç¡®
4. **æ•°æ®åŠ è½½**: âœ… æ­£ç¡®ï¼ˆä¸NeMoä¸€è‡´ï¼‰
5. **DDPé…ç½®**: âœ… æ­£ç¡®ï¼ˆworld_sizeå’Œglobal_rankæ­£ç¡®æ›´æ–°ï¼‰
6. **DataLoaderé…ç½®**: âœ… æ­£ç¡®ï¼ˆä¸NeMoä¸€è‡´ï¼‰
7. **æŸå¤±è®¡ç®—**: âœ… æ­£ç¡®
8. **é…ç½®ä¸€è‡´æ€§**: âœ… ä¸NeMo 100%ä¸€è‡´

### ğŸ¯ å…³é”®å‘ç°

1. **åŒé‡Preprocessorè°ƒç”¨**: è¿™æ˜¯NeMo SSLæ¶æ„çš„å›ºæœ‰è®¾è®¡ï¼Œä¸æ˜¯bugæˆ–æ€§èƒ½é—®é¢˜
2. **DDPé…ç½®**: å·²å®Œå…¨å¯¹é½NeMoï¼Œworld_sizeå’Œglobal_rankæ­£ç¡®æ›´æ–°
3. **æ•°æ®åŠ è½½**: æ‰€æœ‰å‚æ•°ä¸NeMoä¸€è‡´ï¼ˆmax_trial, num_workersç­‰ï¼‰
4. **é…ç½®å‚æ•°**: æ‰€æœ‰é…ç½®é¡¹ä¸NeMoåŸç‰ˆå®Œå…¨ä¸€è‡´

### ğŸ“ å»ºè®®

1. **æ€§èƒ½ä¼˜åŒ–**: å¦‚éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œè¯·å‚è€ƒ`nest_fast-conformer_ddp_example.yaml`ä¸­çš„é«˜çº§DDPé…ç½®
2. **æ•°æ®åŠ è½½**: Linuxç¯å¢ƒä¸‹å¯ä»¥å°†`num_workers`è®¾ç½®ä¸º8ä»¥æé«˜æ€§èƒ½
3. **æ‰¹å¤„ç†å¤§å°**: å¯ä»¥æ ¹æ®GPUå†…å­˜å¢åŠ `batch_size`ï¼ˆå½“å‰ä¸º2ï¼‰

---

**æ£€æŸ¥å®Œæˆ**: æ‰€æœ‰æ¨¡å—é€»è¾‘æ­£ç¡®ï¼Œä¸NeMo 100%å¯¹é½ âœ…

