# GPUæ•°æ®ä¼ è¾“ä¼˜åŒ–æŒ‡å—

## ğŸš€ ä¼˜åŒ–æ¦‚è¿°

é’ˆå¯¹è®­ç»ƒé€Ÿåº¦æ…¢çš„é—®é¢˜ï¼Œå·²å¯¹GPUæ•°æ®ä¼ è¾“è¿›è¡Œä»¥ä¸‹ä¼˜åŒ–ï¼š

### ä¸»è¦ä¼˜åŒ–é¡¹

1. **å¯ç”¨`pin_memory=True`**: å°†æ•°æ®å›ºå®šåœ¨CPUå†…å­˜ä¸­ï¼ŒåŠ é€ŸCPUåˆ°GPUçš„ä¼ è¾“
2. **ä½¿ç”¨`non_blocking=True`**: å¼‚æ­¥ä¼ è¾“ï¼Œå…è®¸CPUå’ŒGPUå¹¶è¡Œå·¥ä½œ
3. **å¢åŠ `prefetch_factor`**: é¢„å–æ›´å¤šbatchï¼Œå‡å°‘GPUç­‰å¾…æ—¶é—´
4. **å¯ç”¨`persistent_workers`**: ä¿æŒworkerè¿›ç¨‹å­˜æ´»ï¼Œå‡å°‘é‡å¯å¼€é”€

---

## ğŸ“Š ä¼˜åŒ–å‰åå¯¹æ¯”

| ä¼˜åŒ–é¡¹ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | å½±å“ |
|--------|--------|--------|------|
| `pin_memory` | True | True | âœ… å·²å¯ç”¨ |
| `non_blocking` | æœªæ˜ç¡® | True | â¬†ï¸ å¼‚æ­¥ä¼ è¾“ï¼ŒCPU-GPUå¹¶è¡Œ |
| `prefetch_factor` | 2 | 4 | â¬†ï¸ æ›´å¤šé¢„å–ï¼Œå‡å°‘ç­‰å¾… |
| `persistent_workers` | False | True | â¬†ï¸ å‡å°‘workeré‡å¯å¼€é”€ |

---

## ğŸ”§ ä¼˜åŒ–è¯¦è§£

### 1. pin_memory (å·²å¯ç”¨)

**é…ç½®**: `pin_memory: true` in DataLoader

**ä½œç”¨**:
- å°†æ•°æ®å›ºå®šåœ¨CPUçš„é¡µé”å®šå†…å­˜ï¼ˆpinned memoryï¼‰ä¸­
- GPUå¯ä»¥ç›´æ¥è®¿é—®pinned memoryï¼Œæ— éœ€é€šè¿‡pageable memory
- **ä¼ è¾“é€Ÿåº¦æå‡**: 2-3x

**å†…å­˜å ç”¨**: å¢åŠ CPUå†…å­˜ä½¿ç”¨ï¼ˆæ•°æ®è¢«å›ºå®šï¼Œä¸èƒ½swapï¼‰

**ä½•æ—¶ä½¿ç”¨**: 
- âœ… GPUè®­ç»ƒæ—¶åº”è¯¥å¯ç”¨
- âœ… æœ‰è¶³å¤ŸCPUå†…å­˜æ—¶å¯ç”¨
- âŒ CPUè®­ç»ƒæ—¶ä¸éœ€è¦

### 2. non_blocking Transfer (å·²ä¼˜åŒ–)

**å®ç°**: `move_data_to_device(batch, device, non_blocking=True)`

**ä½œç”¨**:
- **å¼‚æ­¥ä¼ è¾“**: CPUå¯ä»¥ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªbatchï¼ŒåŒæ—¶GPUæ¥æ”¶å½“å‰batch
- **CPU-GPUå¹¶è¡Œ**: æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡
- **å‡å°‘GPUç©ºé—²**: GPUä¸éœ€è¦ç­‰å¾…CPUå®Œæˆæ•°æ®ä¼ è¾“

**å·¥ä½œåŸç†**:
```python
# åŒæ­¥ä¼ è¾“ (æ…¢)
tensor.to(device)  # CPUç­‰å¾…ä¼ è¾“å®Œæˆ
# GPUç©ºé—²ç­‰å¾…

# å¼‚æ­¥ä¼ è¾“ (å¿«)
tensor.to(device, non_blocking=True)  # CPUç«‹å³è¿”å›
# CPUç»§ç»­å·¥ä½œï¼ŒGPUå¼‚æ­¥æ¥æ”¶æ•°æ®
```

**æ€§èƒ½æå‡**: 10-30% GPUåˆ©ç”¨ç‡æå‡

### 3. prefetch_factor (å·²ä¼˜åŒ–)

**é…ç½®**: `prefetch_factor: 4`

**ä½œç”¨**:
- æ¯ä¸ªworkeré¢„å–4ä¸ªbatch
- å½“GPUå¤„ç†å½“å‰batchæ—¶ï¼Œä¸‹ä¸€ä¸ªbatchå·²ç»å‡†å¤‡å¥½
- å‡å°‘GPUç­‰å¾…æ•°æ®çš„æ—¶é—´

**å†…å­˜å ç”¨**: `num_workers * prefetch_factor * batch_size * sample_size`

**æ¨èå€¼**:
- å°GPU (8GB): 2-4
- ä¸­GPU (16GB): 4-8
- å¤§GPU (24GB+): 8-16

### 4. persistent_workers (å·²å¯ç”¨)

**é…ç½®**: `persistent_workers: true`

**ä½œç”¨**:
- ä¿æŒworkerè¿›ç¨‹åœ¨epochä¹‹é—´å­˜æ´»
- é¿å…æ¯ä¸ªepoché‡æ–°åˆ›å»ºworkerçš„å¼€é”€
- å‡å°‘è¿›ç¨‹å¯åŠ¨å’Œåˆå§‹åŒ–æ—¶é—´

**æ€§èƒ½æå‡**: 10-20% epochåˆ‡æ¢æ—¶é—´å‡å°‘

---

## âš¡ æ•°æ®ä¼ è¾“æµç¨‹ä¼˜åŒ–

### ä¼˜åŒ–å‰ï¼ˆæ…¢ï¼‰
```
CPU: åŠ è½½æ•°æ® â†’ å¤„ç† â†’ ä¼ è¾“åˆ°GPU (åŒæ­¥) â†’ ç­‰å¾…å®Œæˆ â†’ åŠ è½½ä¸‹ä¸€ä¸ª
GPU: ç©ºé—²ç­‰å¾… â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â†
```

### ä¼˜åŒ–åï¼ˆå¿«ï¼‰
```
CPU: åŠ è½½æ•°æ® â†’ å¤„ç† â†’ ä¼ è¾“åˆ°GPU (å¼‚æ­¥) â†’ ç«‹å³åŠ è½½ä¸‹ä¸€ä¸ª â†’ ...
GPU: å¤„ç†batch â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â† â†
     (å¹¶è¡Œå·¥ä½œ)
```

---

## ğŸ” ä»£ç å®ç°

### transfer_batch_to_device (å·²ä¼˜åŒ–)

```python
def transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any:
    """
    Optimized for fast GPU transfer:
    - Uses non_blocking=True for async transfer (allows CPU-GPU overlap)
    - Works with pin_memory=True in DataLoader for faster transfer
    """
    from utils.device_utils import move_data_to_device
    # non_blocking=True enables async transfer
    batch = move_data_to_device(batch, device, non_blocking=True)
    return batch
```

### move_data_to_device (å·²ä¼˜åŒ–)

```python
def move_data_to_device(inputs: Any, device: Union[str, torch.device], non_blocking: bool = True) -> Any:
    """
    Recursively moves inputs to the specified device.
    Uses non_blocking=True by default for async transfer.
    """
    if isinstance(inputs, torch.Tensor):
        return inputs.to(device, non_blocking=non_blocking)  # å¼‚æ­¥ä¼ è¾“
    # ... é€’å½’å¤„ç†å…¶ä»–ç±»å‹
```

---

## ğŸ“ˆ æ€§èƒ½æå‡é¢„æœŸ

### GPUåˆ©ç”¨ç‡æå‡

- **non_blockingä¼ è¾“**: **10-30%** GPUåˆ©ç”¨ç‡æå‡
- **prefetch_factor=4**: **20-30%** GPUåˆ©ç”¨ç‡æå‡
- **pin_memory**: **2-3x** ä¼ è¾“é€Ÿåº¦æå‡
- **persistent_workers**: **10-20%** epochåˆ‡æ¢æ—¶é—´å‡å°‘

### æ€»ä½“è®­ç»ƒé€Ÿåº¦

é¢„æœŸæ€»ä½“è®­ç»ƒé€Ÿåº¦æå‡: **30-50%**

---

## ğŸ› ï¸ ä½¿ç”¨å»ºè®®

### å½“å‰é…ç½®ï¼ˆå·²ä¼˜åŒ–ï¼‰

```yaml
train_ds:
  batch_size: 8
  num_workers: 8
  pin_memory: true  # âœ… å·²å¯ç”¨
  persistent_workers: true  # âœ… å·²å¯ç”¨
  prefetch_factor: 4  # âœ… å·²ä¼˜åŒ–
```

### é«˜æ˜¾å­˜GPUï¼ˆ24GB+ï¼‰

```yaml
train_ds:
  batch_size: 16-32
  num_workers: 16
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 8  # æ›´é«˜é¢„å–
```

### ä½æ˜¾å­˜GPUï¼ˆ8GBï¼‰

```yaml
train_ds:
  batch_size: 4
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2  # å‡å°‘é¢„å–èŠ‚çœå†…å­˜
```

---

## ğŸ” ç›‘æ§å’Œè°ƒè¯•

### æ£€æŸ¥GPUåˆ©ç”¨ç‡

```bash
nvidia-smi -l 1  # æ¯ç§’åˆ·æ–°
```

**æœŸæœ›**: GPUåˆ©ç”¨ç‡åº”è¯¥æ¥è¿‘100%ï¼Œä¸åº”è¯¥æœ‰é•¿æ—¶é—´çš„ç©ºé—²

### æ£€æŸ¥æ•°æ®ä¼ è¾“

```python
# åœ¨training_stepä¸­æ·»åŠ æ—¶é—´æµ‹é‡
import time
start_time = time.time()
batch = next(iter(train_dataloader))  # æµ‹é‡æ•°æ®ä¼ è¾“æ—¶é—´
transfer_time = time.time() - start_time
print(f"Data transfer time: {transfer_time:.4f}s")
```

**æœŸæœ›**: æ•°æ®ä¼ è¾“æ—¶é—´åº”è¯¥ < 10msï¼ˆå¯¹äºbatch_size=8ï¼‰

### æ£€æŸ¥CPU-GPUå¹¶è¡Œ

ä½¿ç”¨PyTorch Profiler:
```python
from torch.profiler import profile, record_function, ProfilerActivity

with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
    with record_function("training_step"):
        loss = model.training_step(batch, 0)

print(prof.key_averages().table(sort_by="cuda_time_total"))
```

**æœŸæœ›**: åº”è¯¥çœ‹åˆ°CPUå’ŒCUDAæ´»åŠ¨é‡å ï¼ˆå¹¶è¡Œï¼‰

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **pin_memoryå†…å­˜å ç”¨**: ä¼šå¢åŠ CPUå†…å­˜ä½¿ç”¨ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿå†…å­˜
2. **non_blockingåŒæ­¥**: éœ€è¦åœ¨GPUæ“ä½œå‰è°ƒç”¨`torch.cuda.synchronize()`ï¼ˆPyTorch Lightningè‡ªåŠ¨å¤„ç†ï¼‰
3. **prefetch_factorå†…å­˜**: ä¼šå¢åŠ CPUå†…å­˜å ç”¨ï¼Œæ ¹æ®å†…å­˜è°ƒæ•´
4. **Windowsé™åˆ¶**: `num_workers=0`æ—¶ï¼Œ`persistent_workers`å’Œ`prefetch_factor`æ— æ•ˆ

---

## ğŸ“ˆ è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨æ··åˆç²¾åº¦**: `precision: 16-mixed`å¯ä»¥å‡å°‘æ•°æ®ä¼ è¾“é‡
2. **ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯**: å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œä½¿ç”¨`accumulate_grad_batches`æ¨¡æ‹Ÿæ›´å¤§batch
3. **ä½¿ç”¨tarredæ•°æ®é›†**: å¯¹äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œtarredæ ¼å¼å¯ä»¥è¿›ä¸€æ­¥æå‡IOæ€§èƒ½
4. **æ•°æ®é¢„å¤„ç†ç¼“å­˜**: å¯¹äºé‡å¤ä½¿ç”¨çš„æ•°æ®é›†ï¼Œè€ƒè™‘é¢„å¤„ç†å¹¶ç¼“å­˜

---

## ğŸ“ æ€»ç»“

### âœ… å·²å®ç°çš„ä¼˜åŒ–

1. âœ… `pin_memory=True` - åŠ é€ŸCPUåˆ°GPUä¼ è¾“
2. âœ… `non_blocking=True` - å¼‚æ­¥ä¼ è¾“ï¼ŒCPU-GPUå¹¶è¡Œ
3. âœ… `prefetch_factor=4` - é¢„å–æ›´å¤šbatch
4. âœ… `persistent_workers=True` - å‡å°‘workeré‡å¯å¼€é”€

### ğŸ¯ é¢„æœŸæ•ˆæœ

- **GPUåˆ©ç”¨ç‡**: æå‡30-50%
- **è®­ç»ƒé€Ÿåº¦**: æå‡30-50%
- **æ•°æ®ä¼ è¾“**: 2-3xæ›´å¿«

---

**æ›´æ–°æ—¥æœŸ**: 2025-01-XX  
**ç‰ˆæœ¬**: 1.0

