# DDPå¹¶è¡Œè°ƒè¯•æŒ‡å—

## ğŸ” é—®é¢˜è¯Šæ–­

### ç—‡çŠ¶
- æ‰“å°/æ—¥å¿—åªæ˜¾ç¤ºä¸€ä¸ªrankçš„è¾“å‡º
- å…¶ä»–rankä¼¼ä¹æ²¡æœ‰è®­ç»ƒ

### å¯èƒ½åŸå› 
1. **æ—¥å¿—é…ç½®**: åªæœ‰rank 0åœ¨æ‰“å°ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼Œä½†éœ€è¦ç¡®è®¤æ‰€æœ‰rankéƒ½åœ¨è®­ç»ƒï¼‰
2. **æ•°æ®åˆ†å¸ƒ**: æŸäº›rankæ²¡æœ‰åˆ†é…åˆ°æ•°æ®
3. **DDPåˆå§‹åŒ–**: DDPæ²¡æœ‰æ­£ç¡®åˆå§‹åŒ–
4. **è®­ç»ƒå¾ªç¯**: æŸäº›rankæ²¡æœ‰è¿›å…¥è®­ç»ƒå¾ªç¯

---

## âœ… æ£€æŸ¥æ¸…å•

### 1. ç¡®è®¤æ‰€æœ‰rankéƒ½åœ¨è®­ç»ƒ

åœ¨`training_step`ä¸­æ·»åŠ è°ƒè¯•è¾“å‡ºï¼š

```python
def training_step(self, batch: ssl_dataset.AudioNoiseBatch, batch_idx: int):
    # è°ƒè¯•: ç¡®è®¤æ‰€æœ‰rankéƒ½åœ¨è®­ç»ƒ
    if batch_idx % 100 == 0:  # æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡
        print(f"[Rank {self.global_rank}] Training step {batch_idx}, batch size: {batch.audio.size(0)}")
    
    # ... æ­£å¸¸è®­ç»ƒä»£ç 
```

**æœŸæœ›**: åº”è¯¥çœ‹åˆ°æ‰€æœ‰rankçš„è¾“å‡ºï¼ˆrank 0, 1, 2, ...ï¼‰

### 2. æ£€æŸ¥æ•°æ®åˆ†å¸ƒ

åœ¨`setup_training_data`ä¸­æ·»åŠ è°ƒè¯•è¾“å‡ºï¼š

```python
def setup_training_data(self, train_data_config):
    # ... è®¾ç½®æ•°æ®åŠ è½½å™¨
    
    if self._trainer is not None:
        print(f"[Rank {self.global_rank}] World size: {self.world_size}, "
              f"Dataset size: {len(self._train_dl.dataset) if hasattr(self._train_dl, 'dataset') else 'N/A'}, "
              f"Batches per rank: {len(self._train_dl) if self._train_dl else 'N/A'}")
```

**æœŸæœ›**: æ¯ä¸ªrankåº”è¯¥çœ‹åˆ°ä¸åŒçš„æ•°æ®é›†å¤§å°ï¼ˆå¦‚æœä½¿ç”¨DistributedSamplerï¼‰

### 3. æ£€æŸ¥DDPåˆå§‹åŒ–

åœ¨`train.py`ä¸­æ·»åŠ ï¼š

```python
@hydra_runner(config_path="config", config_name="nest_fast-conformer")
def main(cfg):
    import torch.distributed as dist
    
    trainer = pl.Trainer(**cfg.trainer)
    
    # æ£€æŸ¥DDPæ˜¯å¦åˆå§‹åŒ–
    if dist.is_available() and dist.is_initialized():
        print(f"[Rank {dist.get_rank()}] DDP initialized. World size: {dist.get_world_size()}")
    else:
        print("[Rank 0] DDP not initialized (single GPU or CPU training)")
    
    # ... ç»§ç»­è®­ç»ƒ
```

### 4. æ£€æŸ¥GPUåˆ©ç”¨ç‡

```bash
# åœ¨è®­ç»ƒæ—¶è¿è¡Œ
nvidia-smi -l 1
```

**æœŸæœ›**: æ‰€æœ‰GPUéƒ½åº”è¯¥æ˜¾ç¤ºä½¿ç”¨ç‡ï¼ˆå¦‚æœä½¿ç”¨å¤šGPUï¼‰

---

## ğŸ”§ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: åªæœ‰rank 0åœ¨æ‰“å°

**åŸå› **: æ—¥å¿—é…ç½®åªå…è®¸rank 0æ‰“å°ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰

**è§£å†³æ–¹æ¡ˆ**: 
- è¿™æ˜¯**æ­£å¸¸è¡Œä¸º**ï¼ŒPyTorch Lightningé»˜è®¤åªä»rank 0æ‰“å°
- å¦‚æœéœ€è¦æ‰€æœ‰rankçš„è¾“å‡ºï¼Œåœ¨ä»£ç ä¸­æ˜ç¡®æ‰“å°ï¼š

```python
def training_step(self, batch, batch_idx):
    # å¼ºåˆ¶æ‰€æœ‰rankéƒ½æ‰“å°
    print(f"[Rank {self.global_rank}] Step {batch_idx}")
    # æˆ–è€…ä½¿ç”¨loggingï¼ˆä¼šè‡ªåŠ¨å¤„ç†rankï¼‰
    logging.info(f"[Rank {self.global_rank}] Step {batch_idx}")
```

### é—®é¢˜2: æŸäº›rankæ²¡æœ‰è®­ç»ƒ

**æ£€æŸ¥**:
1. ç¡®è®¤æ‰€æœ‰rankéƒ½è¿›å…¥äº†`training_step`
2. æ£€æŸ¥æ•°æ®åˆ†å¸ƒæ˜¯å¦æ­£ç¡®
3. æ£€æŸ¥`world_size`æ˜¯å¦æ­£ç¡®è®¾ç½®

**è§£å†³æ–¹æ¡ˆ**:
```python
def training_step(self, batch, batch_idx):
    # æ·»åŠ è°ƒè¯•è¾“å‡º
    if batch_idx == 0:
        print(f"[Rank {self.global_rank}] First training step, batch shape: {batch.audio.shape}")
    
    # æ£€æŸ¥batchæ˜¯å¦ä¸ºç©º
    if batch.audio.size(0) == 0:
        print(f"[Rank {self.global_rank}] WARNING: Empty batch!")
        return None
    
    # ... æ­£å¸¸è®­ç»ƒ
```

### é—®é¢˜3: æ•°æ®åˆ†å¸ƒä¸å‡åŒ€

**æ£€æŸ¥**:
```python
def setup_training_data(self, train_data_config):
    # ... è®¾ç½®æ•°æ®åŠ è½½å™¨
    
    # æ£€æŸ¥æ¯ä¸ªrankçš„æ•°æ®é‡
    if hasattr(self._train_dl, 'dataset'):
        dataset_size = len(self._train_dl.dataset)
        batches_per_rank = len(self._train_dl)
        print(f"[Rank {self.global_rank}] Dataset size: {dataset_size}, "
              f"Batches per rank: {batches_per_rank}, "
              f"World size: {self.world_size}")
```

**è§£å†³æ–¹æ¡ˆ**:
- ç¡®ä¿ä½¿ç”¨`DistributedSampler`ï¼ˆPyTorch Lightningè‡ªåŠ¨å¤„ç†ï¼‰
- æ£€æŸ¥`drop_last`è®¾ç½®ï¼ˆå¦‚æœæ•°æ®ä¸èƒ½å‡åŒ€åˆ†é…ï¼‰

### é—®é¢˜4: DDPæ²¡æœ‰æ­£ç¡®åˆå§‹åŒ–

**æ£€æŸ¥**:
```python
import torch.distributed as dist

def main(cfg):
    trainer = pl.Trainer(**cfg.trainer)
    
    # æ£€æŸ¥DDPçŠ¶æ€
    print(f"DDP available: {dist.is_available()}")
    print(f"DDP initialized: {dist.is_initialized()}")
    if dist.is_initialized():
        print(f"Rank: {dist.get_rank()}, World size: {dist.get_world_size()}")
```

**è§£å†³æ–¹æ¡ˆ**:
- ç¡®ä¿ä½¿ç”¨`strategy="ddp"`æˆ–`strategy="auto"`ï¼ˆå¤šGPUæ—¶ï¼‰
- ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å¯åŠ¨å‘½ä»¤ï¼ˆ`torchrun`æˆ–`python -m torch.distributed.launch`ï¼‰

---

## ğŸš€ è°ƒè¯•ä»£ç æ¨¡æ¿

### åœ¨training_stepä¸­æ·»åŠ è°ƒè¯•

```python
def training_step(self, batch: ssl_dataset.AudioNoiseBatch, batch_idx: int):
    # è°ƒè¯•è¾“å‡ºï¼ˆæ¯Nä¸ªbatchï¼‰
    if batch_idx % 100 == 0:
        print(f"[Rank {self.global_rank}/{self.world_size}] "
              f"Step {batch_idx}, Batch size: {batch.audio.size(0)}, "
              f"Loss device: {batch.audio.device}")
    
    # æ­£å¸¸è®­ç»ƒä»£ç 
    log_probs, encoded_len, masks, tokens = self.forward(...)
    loss_value = self.loss(...)
    
    # æ£€æŸ¥lossæ˜¯å¦æœ‰æ•ˆ
    if torch.isnan(loss_value) or torch.isinf(loss_value):
        print(f"[Rank {self.global_rank}] WARNING: Invalid loss: {loss_value}")
    
    return loss_value
```

### åœ¨setup_training_dataä¸­æ·»åŠ è°ƒè¯•

```python
def setup_training_data(self, train_data_config):
    # æ›´æ–°world_size
    if self._trainer is not None:
        self.world_size = self._trainer.world_size
    
    # è°ƒè¯•è¾“å‡º
    print(f"[Rank {self.global_rank}] Setting up training data. "
          f"World size: {self.world_size}, "
          f"Global rank: {self.global_rank}")
    
    # ... è®¾ç½®æ•°æ®åŠ è½½å™¨
    
    if self._train_dl is not None:
        print(f"[Rank {self.global_rank}] Training dataloader created. "
              f"Batches: {len(self._train_dl)}")
```

---

## ğŸ“Š éªŒè¯DDPæ­£å¸¸å·¥ä½œ

### 1. æ£€æŸ¥æ‰€æœ‰rankéƒ½åœ¨è®­ç»ƒ

è¿è¡Œè®­ç»ƒï¼Œåº”è¯¥çœ‹åˆ°ï¼š
```
[Rank 0/4] Step 0, Batch size: 8
[Rank 1/4] Step 0, Batch size: 8
[Rank 2/4] Step 0, Batch size: 8
[Rank 3/4] Step 0, Batch size: 8
```

### 2. æ£€æŸ¥æ•°æ®åˆ†å¸ƒ

æ¯ä¸ªrankåº”è¯¥å¤„ç†ä¸åŒçš„æ•°æ®ï¼š
- Rank 0: å¤„ç†æ ·æœ¬ 0, 4, 8, ...
- Rank 1: å¤„ç†æ ·æœ¬ 1, 5, 9, ...
- Rank 2: å¤„ç†æ ·æœ¬ 2, 6, 10, ...
- Rank 3: å¤„ç†æ ·æœ¬ 3, 7, 11, ...

### 3. æ£€æŸ¥GPUåˆ©ç”¨ç‡

```bash
nvidia-smi -l 1
```

æ‰€æœ‰GPUéƒ½åº”è¯¥æ˜¾ç¤ºä½¿ç”¨ç‡ã€‚

### 4. æ£€æŸ¥è®­ç»ƒé€Ÿåº¦

å¤šGPUè®­ç»ƒåº”è¯¥æ¯”å•GPUå¿«ï¼ˆæ¥è¿‘çº¿æ€§åŠ é€Ÿï¼‰ï¼š
- 2 GPU: ~1.8-1.9x
- 4 GPU: ~3.5-3.8x
- 8 GPU: ~7.0-7.5x

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ—¥å¿—è¾“å‡º**: é»˜è®¤åªæœ‰rank 0æ‰“å°æ˜¯**æ­£å¸¸çš„**ï¼Œè¿™é¿å…é‡å¤è¾“å‡º
2. **æ•°æ®åˆ†å¸ƒ**: ä½¿ç”¨`DistributedSampler`è‡ªåŠ¨å¤„ç†æ•°æ®åˆ†å¸ƒ
3. **åŒæ­¥**: DDPè‡ªåŠ¨åŒæ­¥æ¢¯åº¦ï¼Œä¸éœ€è¦æ‰‹åŠ¨åŒæ­¥
4. **éªŒè¯**: ä½¿ç”¨`nvidia-smi`æ£€æŸ¥æ‰€æœ‰GPUéƒ½åœ¨ä½¿ç”¨

---

## ğŸ” å¿«é€Ÿè¯Šæ–­å‘½ä»¤

```bash
# 1. æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
nvidia-smi -l 1

# 2. æ£€æŸ¥è¿›ç¨‹
ps aux | grep python

# 3. æ£€æŸ¥DDPè¿›ç¨‹æ•°ï¼ˆåº”è¯¥ç­‰äºGPUæ•°ï¼‰
ps aux | grep python | wc -l
```

---

**æ›´æ–°æ—¥æœŸ**: 2025-01-XX  
**ç‰ˆæœ¬**: 1.0

