# Example configuration file showing DDP strategy with find_unused_parameters
# This is a reference configuration - copy relevant parts to nest_fast-conformer.yaml

# This config demonstrates how to configure DDP strategy with find_unused_parameters
# for PyTorch Lightning 2.0+

# Copy the model section from nest_fast-conformer.yaml
# model:
#   ...

trainer:
  devices: -1  # Use all available GPUs
  num_nodes: 1
  max_epochs: -1
  max_steps: 500000
  val_check_interval: 1.0
  accelerator: gpu
  
  # DDP Strategy configuration optimized for performance (align with NeMo)
  # Note: find_unused_parameters has been removed in PyTorch Lightning 2.0+
  # If you encounter unused parameter errors, ensure all model parameters are used
  # in the forward pass, or the model will automatically handle unused parameters.
  strategy:
    _target_: lightning.pytorch.strategies.DDPStrategy
    gradient_as_bucket_view: true  # Memory optimization: use gradient buckets as views (reduces memory usage, aligns with NeMo)
    static_graph: false  # Set to true if model structure doesn't change (faster, but requires fixed computation graph)
    # Additional optional parameters for further optimization:
    # ddp_comm_hook: null  # Communication hook for gradient synchronization
    # ddp_comm_state: null  # State for communication hook
    # ddp_comm_wrapper: null  # Wrapper for communication hook
    # model_averaging_period: null  # Period for model averaging (null = disabled)
    # bucket_cap_mb: 25  # Maximum bucket size in MB (default: 25, can increase for better performance on large models)
  
  accumulate_grad_batches: 1
  gradient_clip_val: 0.0
  precision: 32
  log_every_n_steps: 10
  enable_progress_bar: True
  num_sanity_val_steps: 0
  check_val_every_n_epoch: 1
  sync_batchnorm: true  # Important for DDP training
  enable_checkpointing: False
  logger: false
  benchmark: false

# Copy exp_manager section from nest_fast-conformer.yaml
# exp_manager:
#   ...

