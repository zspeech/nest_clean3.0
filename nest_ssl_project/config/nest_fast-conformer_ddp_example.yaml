# Example configuration file showing DDP strategy with find_unused_parameters
# This is a reference configuration - copy relevant parts to nest_fast-conformer.yaml

# This config demonstrates how to configure DDP strategy with find_unused_parameters
# for PyTorch Lightning 2.0+

# IMPORTANT: When copying encoder config, make sure to include:
#   sync_max_audio_length: false  # Prevents DDP deadlock in encoder
# See ENCODER_HANGING_ANALYSIS.md for details

# Copy the model section from nest_fast-conformer.yaml
# model:
#   ...

trainer:
  devices: -1  # Use all available GPUs
  num_nodes: 1
  max_epochs: -1
  max_steps: 500000
  val_check_interval: 1.0
  accelerator: gpu
  
  # DDP Strategy configuration (align with NeMo - use simple 'ddp' string)
  # NeMo uses simple 'ddp' or 'auto' strategy, not complex DDPStrategy config
  strategy: ddp  # Use simple string like NeMo examples
  
  accumulate_grad_batches: 1
  gradient_clip_val: 0.0
  precision: 32
  log_every_n_steps: 10
  enable_progress_bar: True
  num_sanity_val_steps: 0
  check_val_every_n_epoch: 1
  sync_batchnorm: true  # Important for DDP training
  enable_checkpointing: False
  logger: false
  benchmark: false

# Copy exp_manager section from nest_fast-conformer.yaml
# exp_manager:
#   ...

